{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Dev Explainer - AI is useful\n",
    "\n",
    "This is a notebook for https://www.aiexplainer.dev/useful\n",
    "\n",
    "View all notebooks at https://github.com/chroma-core/ai_explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI can automate useful tasks**\n",
    "\n",
    "AI is a new type of programming primitive. Large language models (LLMs) let us write software which can process **unstructured** information in a **common sense** way.\n",
    "\n",
    "\n",
    "Consider the task of writing a program to extract a list of people's names from the following paragraph:\n",
    "\n",
    "```plaintext\n",
    "Now the other princes of the Achaeans slept soundly the whole night through, but Agamemnon son of Atreus was troubled, so that he could get no rest. As when fair Hera's lord flashes his lightning in token of great rain or hail or snow when the snow-flakes whiten the ground, or again as a sign that he will open the wide jaws of hungry war, even so did Agamemnon heave many a heavy sigh, for his soul trembled within him. When he looked upon the plain of Troy he marveled at the many watchfires burning in front of Ilion... - The Iliad, Scroll 10\n",
    "```\n",
    "\n",
    "Extracting names is easy for humans, but is very difficult using only traditional programming. Writing a general program to extract names from any paragraph is harder still.\n",
    "\n",
    "However, with an LLM the task becomes almost trivial.\n",
    "\n",
    "What's more, the same instruction can be used to extract names from any paragraph, and the same LLM can be used for a wide variety of tasks. The ability of LLMs to process unstructured information in a common sense way is very general, and can be applied across a wide range of tasks.\n",
    "\n",
    "The next step is to gain some intuitions about how LLMs work in the context of programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an API Key from OpenAI\n",
    "1. Create an OpenAI Account https://platform.openai.com/signup\n",
    "2. Create an API Key https://platform.openai.com/api-keys\n",
    "3. Replace `<your_api_key>` below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code\n",
    "In Colab - you can click, \"Runtime -> \"Run All\" in the topbar\n",
    "\n",
    "In VS Code - you can click \"Run All\" in the topbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "%pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set up the OpenAI API client\n",
    "# Store your key in .env as OPENAI_API_KEY\n",
    "client = openai.OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"You are a helpful assistant that extracts names from text.\"\n",
    "\n",
    "def get_completion(system_prompt: str =\"\", user_prompt: str = \"\"):\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    "    temperature=0.7,\n",
    ")\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paragraph to analyze\n",
    "paragraph = \"\"\"Now the other princes of the Achaeans slept soundly the whole night through, but Agamemnon son of Atreus was troubled, so that he could get no rest. As when fair Hera's lord flashes his lightning in token of great rain or hail or snow when the snow-flakes whiten the ground, or again as a sign that he will open the wide jaws of hungry war, even so did Agamemnon heave many a heavy sigh, for his soul trembled within him. When he looked upon the plain of Troy he marveled at the many watchfires burning in front of Ilion... - The Iliad, Scroll 10\"\"\"\n",
    "\n",
    "# Create the prompt for GPT\n",
    "prompt = f\"\"\"List the names of people in the following paragraph, separated by commas:\n",
    "\n",
    "{paragraph}\n",
    "\n",
    "Only provide the list of names, nothing else.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Call the OpenAI API\n",
    "response = get_completion(\n",
    "    system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "    user_prompt=prompt\n",
    ")\n",
    "\n",
    "# Clean up the names (remove any extra spaces, newlines, etc.)\n",
    "# names = re.sub(r'\\s+', ' ', names).strip()\n",
    "\n",
    "# Print the result\n",
    "# print(f\"{names}\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI's Chat Completions API doesn't support message history \"out of the box,\" so let's quickly modify our helper to add chat memory and demonstrate streaming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def get_completion_with_history_and_streaming(\n",
    "    user_prompt=\"\",\n",
    "):\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    response = get_completion(\n",
    "        user_prompt=json.dumps(chat_history)\n",
    "    )\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return response\n",
    "\n",
    "response = get_completion_with_history_and_streaming(user_prompt=prompt)\n",
    "\n",
    "follow_up = get_completion_with_history_and_streaming(\n",
    "    user_prompt=\"Which person is Atraeus' son? How does the text describe him?\"\n",
    ")\n",
    "\n",
    "print(follow_up)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
