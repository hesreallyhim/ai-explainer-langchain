{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Dev Explainer - AI is useful\n",
    "\n",
    "This is a notebook for https://www.aiexplainer.dev/useful\n",
    "\n",
    "View all notebooks at https://github.com/chroma-core/ai_explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an API Key from Google AI Studio\n",
    "1. Go to Google AI Studio: https://aistudio.google.com\n",
    "2. Click on 'Get API key' in the left sidebar\n",
    "3. Create a new API key or use an existing one\n",
    "4. Replace `<your_api_key>` below with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code\n",
    "In Colab - you can click, \"Runtime\" -> \"Run All\" in the topbar\n",
    "\n",
    "In VS Code - you can click \"Run All\" in the topbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "%pip install google-genai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "# Make sure your GOOGLE_API_KEY is set in your .env variable\n",
    "client = genai.Client()\n",
    "\n",
    "MODEL_NAME = \"gemini-2.0-flash\"\n",
    "\n",
    "def make_content(\n",
    "    role: str,\n",
    "    text: str,\n",
    ") -> types.Content:\n",
    "  \"\"\"\n",
    "  Create a content object for the Gemini API.\n",
    "  \"\"\"\n",
    "  content = types.Content(\n",
    "      role=role,\n",
    "      parts=[\n",
    "          types.Part.from_text(text=text)\n",
    "      ],\n",
    "  )\n",
    "  return content\n",
    "\n",
    "# Define chat helper function\n",
    "def get_completion(system_prompt: str =\"\", user_prompt: str = \"\"):\n",
    "  response = client.models.generate_content(\n",
    "      model=MODEL_NAME,\n",
    "      contents=[make_content(\"user\", user_prompt)],\n",
    "      config={\n",
    "        \"system_instruction\": system_prompt,\n",
    "        \"temperature\": 0.7,\n",
    "      }\n",
    "  )\n",
    "  return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paragraph to analyze\n",
    "paragraph = \"\"\"Now the other princes of the Achaeans slept soundly the whole night through, but Agamemnon son of Atreus was troubled, so that he could get no rest. As when fair Hera's lord flashes his lightning in token of great rain or hail or snow when the snow-flakes whiten the ground, or again as a sign that he will open the wide jaws of hungry war, even so did Agamemnon heave many a heavy sigh, for his soul trembled within him. When he looked upon the plain of Troy he marveled at the many watchfires burning in front of Ilion... - The Iliad, Scroll 10\"\"\"\n",
    "\n",
    "# Create the prompt for Gemini\n",
    "prompt = f\"\"\"List the names of all the people mentioned in the following paragraph, separated by commas:\n",
    "\n",
    "<PARAGRAPH>\n",
    "{paragraph}\n",
    "</PARAGRAPH>\n",
    "\n",
    "Only provide the list of names, nothing else, in the format:\n",
    "<FORMAT>\n",
    "<NAME_1>, <NAME_2>, <NAME_3>, ...\n",
    "</FORMAT>\n",
    "\"\"\"\n",
    "\n",
    "# Get the completion from Gemini\n",
    "response = get_completion(\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    user_prompt=prompt\n",
    ")\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was pretty complicated! Fortunately, Google provides another module for chat interactions. Let's try that again using the chats module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = client.chats.create(model='gemini-1.5-flash')\n",
    "response = chat.send_message(message=prompt, config={\n",
    "    \"temperature\": 0.7,\n",
    "    \"candidate_count\": 1,\n",
    "})\n",
    "try:\n",
    "    print(response.candidates[0].content.parts[0].text) # type: ignore\n",
    "except AttributeError:\n",
    "    print(\"Error retrieving response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Let's see it once more, this time with streaming enabled, and message history as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = client.chats.create(model='gemini-1.5-flash', history=[])\n",
    "response = chat.send_message(prompt, config={\n",
    "    \"temperature\": 0.7,\n",
    "    \"candidate_count\": 1,\n",
    "})\n",
    "try:\n",
    "    followup = \"Which person is Atraeus' son? How does the text describe him?\"\n",
    "    streaming_response = chat.send_message_stream(message=followup, config={\n",
    "        \"temperature\": 0.7,\n",
    "        \"candidate_count\": 1,\n",
    "    })\n",
    "    for chunk in streaming_response:\n",
    "        print(chunk.candidates[0].content.parts[0].text, end=\"\", flush=True) # type: ignore\n",
    "except AttributeError:\n",
    "    print(\"Error retrieving response\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
